{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMI0JT_YuYF3"
      },
      "source": [
        "## Exercício: Modelo de Linguagem (Bengio 2003) - MLP + Embeddings\n",
        "\n",
        "Neste exercício iremos treinar uma rede neural similar a do Bengio 2003 para prever a próxima palavra de um texto, data as palavras anteriores como entrada. Esta tarefa é chamada de \"Modelagem da Linguagem\".\n",
        "\n",
        "Portanto, você deve implementar o modelo de linguagem inspirado no artigo do Bengio, para prever a próxima palavra usando rede com embeddings e duas camadas.\n",
        "Sugestão de alguns parâmetros:\n",
        "* context_size = 9\n",
        "* max_vocab_size = 3000\n",
        "* embedding_dim = 64\n",
        "* usar pontuação no vocabulário\n",
        "* descartar qualquer contexto ou target que não esteja no vocabulário\n",
        "* É esperado conseguir uma perplexidade da ordem de 50.\n",
        "* Procurem fazer asserts para garantir que partes do seu programa estão testadas\n",
        "\n",
        "Este enunciado não é fixo, podem mudar qualquer um dos parâmetros acima, mas procurem conseguir a perplexidade esperada ou menor.\n",
        "\n",
        "Gerem alguns frases usando um contexto inicial e depois deslocando o contexto e prevendo a próxima palavra gerando frases compridas para ver se está gerando texto plausível.\n",
        "\n",
        "Algumas dicas:\n",
        "- Inclua caracteres de pontuação (ex: `.` e `,`) no vocabulário.\n",
        "- Deixe tudo como caixa baixa (lower-case).\n",
        "- A escolha do tamanho do vocabulario é importante: ser for muito grande, fica difícil para o modelo aprender boas representações. Se for muito pequeno, o modelo apenas conseguirá gerar textos simples.\n",
        "- Remova qualquer exemplo de treino/validação/teste que tenha pelo menos um token desconhecido (ou na entrada ou na saída).\n",
        "- Durante a depuração, faça seu dataset ficar bem pequeno, para que a depuração seja mais rápida e não precise de GPU. Somente ligue a GPU quando o seu laço de treinamento já está funcionando\n",
        "- Não deixe para fazer esse exercício na véspera. Ele é trabalhoso.\n",
        "\n",
        "Procure por `TODO` para entender onde você precisa inserir o seu código."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYbkEzdD37sZ"
      },
      "source": [
        "## Faz download e carrega o dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists(\"data\"):\n",
        "    os.mkdir(\"data\")\n",
        "    !wget https://www.gutenberg.org/ebooks/67724.txt.utf-8 -P data/\n",
        "    !wget https://www.gutenberg.org/ebooks/67725.txt.utf-8 -P data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qAnqY_q0beK",
        "outputId": "f810fdb0-138d-4917-b7ef-69ab266acef6"
      },
      "outputs": [],
      "source": [
        "text = (\n",
        "    open(\"data/67724.txt.utf-8\", \"r\").read() + open(\"data/67725.txt.utf-8\", \"r\").read()\n",
        ")\n",
        "\n",
        "paragraphs = text.split(\"\\n\\n\")\n",
        "len(paragraphs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhUFjtNdDuG0",
        "outputId": "78798c0c-deca-4454-d3fb-7d3ba70f3e91"
      },
      "outputs": [],
      "source": [
        "cleaned_paragraphs = [\n",
        "    paragraph.replace(\"\\n\", \" \") for paragraph in paragraphs if paragraph.strip()\n",
        "]\n",
        "len(cleaned_paragraphs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFVN2ihb33Rf"
      },
      "source": [
        "## Análise do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSRHqe3H4ZFw",
        "outputId": "4a985c7a-ce1d-4b72-d253-c9fbbc5f9440"
      },
      "outputs": [],
      "source": [
        "# Conta as palavras no dataset\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "regular_expression = re.compile(r\"\\w+|[.,!?-]\")\n",
        "\n",
        "\n",
        "def count_words(texts):\n",
        "    word_counts = Counter()\n",
        "    for text in texts:\n",
        "        word_counts.update(re.findall(regular_expression, text.lower()))\n",
        "    return word_counts\n",
        "\n",
        "\n",
        "word_counts = count_words(cleaned_paragraphs)\n",
        "\n",
        "print(len(word_counts))\n",
        "print(word_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyGVDL9KzJ_I"
      },
      "source": [
        "## Criando um vocabulário"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiP7OCo9zJ_I"
      },
      "outputs": [],
      "source": [
        "vocab_size = 3000\n",
        "most_frequent_words = [\"<unk>\"] + [\n",
        "    word for word, count in word_counts.most_common(vocab_size)\n",
        "]\n",
        "vocab = {word: i for i, word in enumerate(most_frequent_words)}\n",
        "vocab_size += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhbhAsZbzJ_J",
        "outputId": "6a53c9e0-308d-4082-e225-cfa376e8f39a"
      },
      "outputs": [],
      "source": [
        "def encode_sentence(sentence, vocab):\n",
        "    return [vocab.get(word, 0) for word in re.findall(r\"\\w+|[.,!?-]\", sentence.lower())]\n",
        "\n",
        "\n",
        "def decode_sentence(sentence, most_frequent_words):\n",
        "    return \" \".join([most_frequent_words[c] for c in sentence])\n",
        "\n",
        "\n",
        "text = cleaned_paragraphs[0]\n",
        "\n",
        "code = encode_sentence(text, vocab)\n",
        "decode = decode_sentence(code, most_frequent_words)\n",
        "\n",
        "print(code)\n",
        "print(decode)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wia_ygbvzJ_J"
      },
      "source": [
        "## Classe do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iy-elI1magRR"
      },
      "outputs": [],
      "source": [
        "context_size = 5  # 5 palavras de entrada. O target é a próxima palavra\n",
        "max_vocab_size = 3000\n",
        "embedding_dim = 64\n",
        "debug = 1064"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1aetOpmDeQd"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, val = train_test_split(cleaned_paragraphs, test_size=0.2, random_state=18)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD1CVci2zJ_J",
        "outputId": "5bf0839e-f30e-4ff2-ed6f-4f3fda782b7c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "torch.manual_seed(18)\n",
        "\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data, vocab, context_size):\n",
        "        self.data = data\n",
        "        self.vocab = vocab\n",
        "        self.context_size = context_size\n",
        "        self.x = []\n",
        "        self.y = []\n",
        "        for sentence in data:\n",
        "            words = encode_sentence(sentence, vocab)\n",
        "            len_sample = len(words)\n",
        "            if len_sample < context_size:\n",
        "                continue\n",
        "            for i in range(len(words) - context_size):\n",
        "                if 0 in words[i : i + context_size] or 0 == words[i + context_size]:\n",
        "                    continue\n",
        "                self.x.append(words[i : i + context_size])\n",
        "                self.y.append(words[i + context_size])\n",
        "\n",
        "        self.x = torch.tensor(self.x)\n",
        "        self.y = torch.tensor(self.y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.y[idx]\n",
        "\n",
        "\n",
        "# if debug limit data to 10 samples\n",
        "if debug:\n",
        "    train = train[:debug]\n",
        "    val = val[:debug]\n",
        "\n",
        "train_data = MyDataset(train, vocab, context_size)\n",
        "val_data = MyDataset(val, vocab, context_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "train_loader = DataLoader(\n",
        "    train_data, batch_size=batch_size, shuffle=True, drop_last=True\n",
        ")\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5_-Yud0zJ_K"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2qKG9YczJ_K"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class LanguageModel(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size, hidden=128):\n",
        "        super(LanguageModel, self).__init__()\n",
        "\n",
        "        # Look up table\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Linear layer\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, hidden)\n",
        "        self.relu = nn.ReLU()\n",
        "        # Linear layer\n",
        "        self.linear2 = nn.Linear(hidden, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if len(x.shape) > 2:\n",
        "            x = x.view(x.shape[0], -1)\n",
        "        if len(x.shape) == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "        x = self.embedding(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yjQ1KXOzJ_K"
      },
      "outputs": [],
      "source": [
        "model = LanguageModel(\n",
        "    vocab_size, embedding_dim=128, context_size=context_size, hidden=256\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# number of parameters\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UngUhyu7zJ_L"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initial Loss and PPL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "lr = 1e-3\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "total_loss = 0\n",
        "with torch.no_grad():\n",
        "    accuracy = 0\n",
        "    for _input, target in val_loader:\n",
        "        _input, target = _input.to(device), target.to(device)\n",
        "        output = model(_input)\n",
        "        loss = criterion(output, target)\n",
        "        accuracy += (output.argmax(dim=1) == target).sum().item()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "print(f\"Initial Val Loss: {total_loss/len(val_loader)}\")\n",
        "print(f\"Initial Val Accuracy: {accuracy/len(val_loader):.3f}%\")\n",
        "perplexity = torch.exp(torch.tensor(total_loss / len(val_loader)))\n",
        "print(f\"Initial Val Perplexity: {perplexity.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    accuracy = 0\n",
        "    for i, (_input, target) in enumerate(train_loader):\n",
        "        _input, target = _input.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(_input)\n",
        "        loss = criterion(output, target)\n",
        "        total_loss += loss.item()\n",
        "        accuracy += (output.argmax(dim=1) == target).sum()/batch_size\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    # mean accuracy by the number of batches\n",
        "    accuracy /= \n",
        "    perplexity = torch.exp(torch.tensor(total_loss/len(train_loader)))\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\\nTrain Loss: {total_loss/len(train_loader)}, Train Perplexity: {perplexity.item()}, Train Accuracy: {accuracy/len(train_loader):.3f}%\")\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    accuracy = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (_input, target) in enumerate(val_loader):\n",
        "            _input, target = _input.to(device), target.to(device)\n",
        "            output = model(_input)\n",
        "            loss = criterion(output, target)\n",
        "            accuracy += (output.argmax(dim=1) == target).sum().item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    perplexity = torch.exp(torch.tensor(total_loss/len(val_loader)))\n",
        "    print(f\"Validation Loss: {total_loss/len(val_loader)}, Validation Perplexity: {perplexity.item()}, Validation Accuracy: {accuracy/len(val_loader):.3f}%\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSXfwYISDoPN"
      },
      "source": [
        "## Avaliação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXXO78GSDqPg"
      },
      "outputs": [],
      "source": [
        "\"\"\" TODO: calcule a perplexidade final no dataset de validação \"\"\"\n",
        "\n",
        "# Perplexidade final\n",
        "\n",
        "model.eval()\n",
        "total_loss = 0\n",
        "with torch.no_grad():\n",
        "    for i, (_input, target) in enumerate(val_loader):\n",
        "        _input, target = _input.to(device), target.to(device)\n",
        "        output = model(_input)\n",
        "        loss = criterion(output, target)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "perplexity = torch.exp(torch.tensor(total_loss / len(val_loader)))\n",
        "print(f\"Final Perplexity: {perplexity.item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1zhxVqfzJ_M"
      },
      "source": [
        "## Exemplo de uso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PExkoWOzJ_M"
      },
      "outputs": [],
      "source": [
        "text = \"um dia a praia irá\"\n",
        "\n",
        "\n",
        "def generate_text(model, vocab, text, max_length):\n",
        "    \"\"\"TODO: implemente a função para gerar texto até atingir o max_length\"\"\"\n",
        "    model.eval()\n",
        "    words = text.split(\" \")\n",
        "    for i in range(max_length):\n",
        "        input_ids = encode_sentence(\" \".join(text.split()[-context_size:]), vocab)\n",
        "        _input = torch.tensor([input_ids]).to(device)\n",
        "        output = model(_input)\n",
        "        word = decode_sentence([output.argmax(dim=1).item()], most_frequent_words)\n",
        "        words.append(word)\n",
        "    return \" \".join(words)\n",
        "\n",
        "\n",
        "context = 5\n",
        "max_length = 15\n",
        "generate_text(model, vocab, text, max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_loader"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
