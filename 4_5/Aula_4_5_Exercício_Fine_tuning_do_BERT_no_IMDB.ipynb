{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OG5DT_dm6mk"
   },
   "source": [
    "# Notebook de referência\n",
    "\n",
    "Nome:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZ80hHaftwUd"
   },
   "source": [
    "## Instruções:\n",
    "\n",
    "\n",
    "Treinar e medir a acurácia de um modelo BERT (ou variantes) para classificação binária usando o dataset do IMDB (20k/5k amostras de treino/validação).\n",
    "\n",
    "Importante:\n",
    "- Deve-se implementar o próprio laço de treinamento.\n",
    "- Implementar o acumulo de gradiente.\n",
    "\n",
    "Dicas:\n",
    "- BERT geralmente costuma aprender bem uma tarefa com poucas épocas (de 3 a 5 épocas). Se tiver demorando mais de 5 épocas para chegar em 80% de acurácia, ajuste os hiperparametros.\n",
    "\n",
    "- Solução para erro de memória:\n",
    "  - Usar bfloat16 permite quase dobrar o batch size\n",
    "\n",
    "Opcional:\n",
    "- Pode-se usar a função trainer da biblioteca Transformers/HuggingFace para verificar se seu laço de treinamento está correto. Note que ainda assim é obrigatório implementar o laço próprio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhpAkifICdJo"
   },
   "source": [
    "# Fixando a seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ozXD-xYCcrT"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wHeZ9nAOEB0U",
    "outputId": "bdd4a1f7-e1d0-4377-9638-a4ee1e968a38"
   },
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXFdJz2KVeQw"
   },
   "source": [
    "## Preparando Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHMi_Kq65fPM"
   },
   "source": [
    "Primeiro, fazemos download do dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2wbnfzst5O3k",
    "outputId": "bebda5c0-5614-4cd0-a2f4-5754cdb9c336"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# extract tar.gz\n",
    "import tarfile\n",
    "\n",
    "\n",
    "def unzip_tar_gz(file_path, dest_dir):\n",
    "    with tarfile.open(file_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=dest_dir)\n",
    "\n",
    "\n",
    "if not os.path.exists(\"data/aclImdb\"):\n",
    "\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "    # Download the dataset\n",
    "    !wget -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz -P data\n",
    "\n",
    "    # Extract the dataset\n",
    "    unzip_tar_gz(\"data/aclImdb_v1.tar.gz\", \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Giyi5Rv_NIm"
   },
   "source": [
    "## Carregando o dataset\n",
    "\n",
    "Criaremos uma divisão de treino (20k exemplos) e validação (5k exemplos) artificialmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0HIN_xLI_TuT",
    "outputId": "787fc595-88b1-486a-8c0c-bcde36396793"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "max_valid = 5000\n",
    "\n",
    "\n",
    "def load_texts(folder):\n",
    "    texts = []\n",
    "    for path in os.listdir(folder):\n",
    "        with open(\n",
    "            os.path.join(folder, path), encoding=\"utf-8\"\n",
    "        ) as f:  # Especificando o codec como UTF-8\n",
    "            texts.append(f.read())\n",
    "    return texts\n",
    "\n",
    "\n",
    "x_train_pos = load_texts(\"data/aclImdb/train/pos\")\n",
    "x_train_neg = load_texts(\"data/aclImdb/train/neg\")\n",
    "x_test_pos = load_texts(\"data/aclImdb/test/pos\")\n",
    "x_test_neg = load_texts(\"data/aclImdb/test/neg\")\n",
    "\n",
    "x_train = x_train_pos + x_train_neg\n",
    "x_test = x_test_pos + x_test_neg\n",
    "y_train = [1] * len(x_train_pos) + [0] * len(x_train_neg)\n",
    "y_test = [1] * len(x_test_pos) + [0] * len(x_test_neg)\n",
    "\n",
    "# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n",
    "c = list(zip(x_train, y_train))\n",
    "random.shuffle(c)\n",
    "x_train, y_train = zip(*c)\n",
    "\n",
    "x_valid = x_train[-max_valid:]\n",
    "y_valid = y_train[-max_valid:]\n",
    "x_train = x_train[:-max_valid]\n",
    "y_train = y_train[:-max_valid]\n",
    "\n",
    "print(len(x_train), \"amostras de treino.\")\n",
    "print(len(x_valid), \"amostras de desenvolvimento.\")\n",
    "print(len(x_test), \"amostras de teste.\")\n",
    "\n",
    "print(\"3 primeiras amostras treino:\")\n",
    "for x, y in zip(x_train[:3], y_train[:3]):\n",
    "    print(y, x[:100])\n",
    "\n",
    "print(\"3 últimas amostras treino:\")\n",
    "for x, y in zip(x_train[-3:], y_train[-3:]):\n",
    "    print(y, x[:100])\n",
    "\n",
    "print(\"3 primeiras amostras validação:\")\n",
    "for x, y in zip(x_valid[:3], y_test[:3]):\n",
    "    print(y, x[:100])\n",
    "\n",
    "print(\"3 últimas amostras validação:\")\n",
    "for x, y in zip(x_valid[-3:], y_valid[-3:]):\n",
    "    print(y, x[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "n_classes = len(set(y_train))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sagui-nlp/debertinha-ptbr-xsmall\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"sagui-nlp/debertinha-ptbr-xsmall\", num_labels=n_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lenght = model.config.max_position_embeddings\n",
    "\n",
    "x_train_tokenized = tokenizer(\n",
    "    x_train, padding=True, truncation=True, max_length=max_lenght, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "x_valid_tokenized = tokenizer(\n",
    "    x_valid, padding=True, truncation=True, max_length=max_lenght, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "x_test_tokenized = tokenizer(\n",
    "    x_test, padding=True, truncation=True, max_length=max_lenght, return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_dict({**x_train_tokenized, \"label\": y_train})\n",
    "valid_dataset = Dataset.from_dict({**x_valid_tokenized, \"label\": y_valid})\n",
    "test_dataset = Dataset.from_dict({**x_test_tokenized, \"label\": y_test})\n",
    "\n",
    "train_dataset.set_format(\n",
    "    \"pt\", columns=[\"input_ids\", \"attention_mask\", \"label\"], output_all_columns=True\n",
    ")\n",
    "valid_dataset.set_format(\n",
    "    \"pt\", columns=[\"input_ids\", \"attention_mask\", \"label\"], output_all_columns=True\n",
    ")\n",
    "test_dataset.set_format(\n",
    "    \"pt\", columns=[\"input_ids\", \"attention_mask\", \"label\"], output_all_columns=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "accumulation_steps = 4\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulation_steps = 4  # Defina o número de passos de acumulação desejados\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss = (\n",
    "            loss / accumulation_steps\n",
    "        )  # Divida a perda pelo número de passos de acumulação\n",
    "        loss.backward()\n",
    "\n",
    "        if (step + 1) % accumulation_steps == 0 or step == len(train_dataloader) - 1:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()  # Zera os gradientes apenas depois de fazer a atualização\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss_valid = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for batch in valid_dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss_valid += loss.item()\n",
    "            total += len(labels)\n",
    "            correct += (outputs.logits.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{epochs} - Train Loss: {total_loss/len(train_dataloader):.4f} - Valid Loss: {total_loss_valid/len(valid_dataloader):.4f} - Accuracy: {correct/total:.4f}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
